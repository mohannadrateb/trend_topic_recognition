{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We use word embeddings to calculate a relevence score for each tweet. This quantifies how relevant each tweet is to the main themes."
      ],
      "metadata": {
        "id": "5REMrWOsVTR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flair"
      ],
      "metadata": {
        "id": "QGxT-s4t5v61",
        "outputId": "50395479-56ef-4005-fc11-79299408edc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.10-py3-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.10.0+cu111)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (1.0.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 59.6 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 28.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (3.0.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 42.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=7cc48292609610ef52287b77450924c783524a4b39bfe1d9d69b2929bc1ec7c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=5f54151d538e71d367834532004b97d6b19a107d7cc14e142c8cb426f2068b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=d5a77daefd0a931c8abbb9711ff7f3e6f848b839f7bdbde6c6d721bdd64cd202\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14393 sha256=5981c427aeef3376c73266477229b0aa18949139eb2e7dd3cba62345ce908be9\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=fa4a3f1e3d680e7e601a078e548f334232939f51a697d0fc4acbc2351c077c14\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=bbd31e22610c6e99cfedca27ec42bd934165e63d78294800ba7cae1a617a83c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=dd69e24fb5b27bcb2da0cf4b6719f80ceffc36aa6442c4711029c6ae65a8a3ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.10.1\n",
            "    Uninstalling importlib-metadata-4.10.1:\n",
            "      Successfully uninstalled importlib-metadata-4.10.1\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.4.0 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-6.0 requests-2.27.1 sacremoses-0.0.47 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.11.4 transformers-4.16.0 wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vc-4RX6U5aOX",
        "outputId": "9b32fd52-1aea-4142-ff0b-8e3c783c3fa1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:02:21,190 https://flair.informatik.hu-berlin.de/resources/embeddings/token/de-crawl-fasttext-300d-1M.vectors.npy not found in cache, downloading to /tmp/tmpgm9z0pvf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1199998928/1199998928 [01:23<00:00, 14376257.97B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:03:45,099 copying /tmp/tmpgm9z0pvf to cache at /root/.flair/embeddings/de-crawl-fasttext-300d-1M.vectors.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:03:49,443 removing temp file /tmp/tmpgm9z0pvf\n",
            "2022-01-28 11:03:51,246 https://flair.informatik.hu-berlin.de/resources/embeddings/token/de-crawl-fasttext-300d-1M not found in cache, downloading to /tmp/tmpi92ozx53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42677439/42677439 [00:02<00:00, 15918538.71B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:03:54,338 copying /tmp/tmpi92ozx53 to cache at /root/.flair/embeddings/de-crawl-fasttext-300d-1M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:03:54,393 removing temp file /tmp/tmpi92ozx53\n"
          ]
        }
      ],
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "from flair.data import Sentence\n",
        "\n",
        "# initiate embedding class WordEmbeddings\n",
        "glove_embedding = WordEmbeddings('de-crawl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ebrs7w2H5aOa",
        "outputId": "6991d7d2-9fa8-44d1-e07a-bb7b46f7d1a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:11:29,152 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpfw9wpj6v\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160000128/160000128 [00:09<00:00, 17173250.59B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:11:38,887 copying /tmp/tmpfw9wpj6v to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:11:39,221 removing temp file /tmp/tmpfw9wpj6v\n",
            "2022-01-28 11:11:39,990 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmppolwh2_m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21494764/21494764 [00:01<00:00, 11963638.44B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-28 11:11:42,184 copying /tmp/tmppolwh2_m to cache at /root/.flair/embeddings/glove.gensim\n",
            "2022-01-28 11:11:42,213 removing temp file /tmp/tmppolwh2_m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# english words embedding\n",
        "en_embedding = WordEmbeddings('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GPuf0urI5aOa"
      },
      "outputs": [],
      "source": [
        "def embed(word):\n",
        "    # create sentence.\n",
        "    sentence = Sentence(word)\n",
        "\n",
        "    # embed a sentence using glove.\n",
        "    glove_embedding.embed(sentence)\n",
        "\n",
        "    # now check out the embedded tokens.\n",
        "    for token in sentence:\n",
        "        #print(token)\n",
        "        #print(token.embedding)\n",
        "        torch_tensor = token.embedding\n",
        "        np_arr = torch_tensor.cpu().detach().numpy()\n",
        "        return(np_arr)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k6TrzAeD5aOb"
      },
      "outputs": [],
      "source": [
        "def embed_en(word):\n",
        "    # create sentence.\n",
        "    sentence = Sentence(word)\n",
        "\n",
        "    # embed a sentence using glove.\n",
        "    en_embedding.embed(sentence)\n",
        "\n",
        "    # now check out the embedded tokens.\n",
        "    for token in sentence:\n",
        "        #print(token)\n",
        "        #print(token.embedding)\n",
        "        torch_tensor = token.embedding\n",
        "        np_arr = torch_tensor.cpu().detach().numpy()\n",
        "        return(np_arr) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Criz7ht15aOc",
        "outputId": "a5ddd174-7786-4fb7-cc24-adb2f0383ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0312,  0.0246,  0.0563, -0.    ,  0.0175, -0.06  , -0.0356,\n",
              "       -0.0589,  0.0037,  0.0257, -0.0082, -0.048 ,  0.0091, -0.0106,\n",
              "       -0.0848, -0.1172,  0.0391, -0.0382, -0.0252, -0.0812,  0.0134,\n",
              "        0.0119,  0.027 ,  0.015 ,  0.0026,  0.0277,  0.0092,  0.0222,\n",
              "        0.0159,  0.002 ,  0.0974, -0.0257, -0.0019, -0.0929,  0.0111,\n",
              "       -0.0111, -0.0229,  0.1161, -0.0345, -0.1042,  0.0099, -0.054 ,\n",
              "        0.0373, -0.0115, -0.0523, -0.0097,  0.0188,  0.0204,  0.0048,\n",
              "       -0.0762, -0.0294, -0.0072, -0.0143, -0.0305, -0.0112,  0.0083,\n",
              "       -0.0064,  0.0513, -0.0103, -0.102 , -0.0358, -0.0458, -0.0349,\n",
              "        0.0992,  0.0205, -0.0247,  0.0157, -0.0332,  0.009 , -0.1691,\n",
              "       -0.0371, -0.0963, -0.0576,  0.0081, -0.0069,  0.0516,  0.041 ,\n",
              "        0.0151,  0.0144, -0.0216,  0.0075,  0.0487, -0.0406, -0.0249,\n",
              "       -0.05  ,  0.1426, -0.0039, -0.0336,  0.0233, -0.0105,  0.0521,\n",
              "       -0.0111, -0.0382,  0.0133,  0.0094,  0.0949, -0.0411, -0.0459,\n",
              "        0.0366, -0.0104, -0.0681,  0.0263,  0.0282, -0.0437,  0.0337,\n",
              "       -0.0886,  0.0833,  0.0591, -0.0179, -0.0341, -0.0795, -0.0297,\n",
              "       -0.0136,  0.0484, -0.0286, -0.0006,  0.0382,  0.0124,  0.0391,\n",
              "       -0.0264,  0.002 ,  0.0577,  0.005 , -0.0272,  0.0909,  0.0731,\n",
              "       -0.1287, -0.0497, -0.0231,  0.0151,  0.0283,  0.0823,  0.0858,\n",
              "       -0.0419,  0.0569, -0.0159, -0.1065,  0.0483, -0.006 , -0.0076,\n",
              "        0.0393, -0.1747,  0.0414, -0.0384,  0.0136,  0.0011,  0.0433,\n",
              "        0.079 ,  0.0135,  0.0253,  0.0161, -0.0086, -0.0015, -0.0062,\n",
              "        0.0653,  0.0642, -0.0209, -0.0019,  0.0139,  0.0651, -0.0021,\n",
              "        0.0092,  0.0342, -0.0504, -0.0106,  0.048 , -0.008 ,  0.0036,\n",
              "        0.0526, -0.0184, -0.0748, -0.0344,  0.0342, -0.005 ,  0.0145,\n",
              "        0.0649,  0.0485, -0.0649, -0.1428, -0.0547, -0.0583, -0.004 ,\n",
              "       -0.0006, -0.0088, -0.0169, -0.0031,  0.095 ,  0.0281,  0.0118,\n",
              "        0.0121,  0.0038, -0.072 ,  0.0359,  0.0468,  0.0019,  0.1453,\n",
              "        0.0528,  0.0438, -0.0013, -0.0182, -0.0172,  0.0484, -0.0419,\n",
              "       -0.0097, -0.0214,  0.0242, -0.0335, -0.037 , -0.064 ,  0.0096,\n",
              "        0.0335,  0.0002,  0.0151,  0.0422,  0.0769,  0.0018, -0.0488,\n",
              "        0.0112,  0.0388, -0.0594,  0.0237,  0.0567, -0.0222,  0.007 ,\n",
              "       -0.0577,  0.056 , -0.0225, -0.0215,  0.0049, -0.0209,  0.0114,\n",
              "       -0.0463, -0.0025,  0.0077, -0.031 ,  0.043 , -0.0093, -0.035 ,\n",
              "        0.0875, -0.0294,  0.0175,  0.0272,  0.0008, -0.009 ,  0.0122,\n",
              "        0.0587, -0.0452, -0.027 , -0.0701, -0.1092,  0.038 ,  0.0366,\n",
              "        0.0296, -0.0711,  0.0003,  0.0199,  0.0263,  0.0345,  0.0298,\n",
              "        0.0096,  0.0226,  0.0079, -0.0768, -0.0049,  0.0297, -0.0385,\n",
              "       -0.1689, -0.0554, -0.04  ,  0.065 , -0.0539, -0.0188, -0.0989,\n",
              "        0.0383, -0.0623, -0.1054,  0.0891,  0.1049, -0.0642,  0.0287,\n",
              "       -0.0159,  0.0499,  0.0381,  0.0074, -0.0222,  0.0284,  0.0087,\n",
              "       -0.0491, -0.0543,  0.0834,  0.0813, -0.0232, -0.0336,  0.0044,\n",
              "        0.0357, -0.0105,  0.0571, -0.0355, -0.0302,  0.0523],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# To check the method embed with an example\n",
        "embed('Alles')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "dA8xZnsHZFRa",
        "outputId": "8c9f263d-1ed5-40f0-ef29-9695984c0199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "w0JdTvx8ZG0E",
        "outputId": "03cf8cb2-f853-4ba2-fb1f-ecf99431167d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ekqPaLZD5aOc",
        "outputId": "a73cd335-12f3-4e70-cd89-ee9a07ef5339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75.24"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#gloveFile = \"glove.6B.50d.txt\"\n",
        "import numpy as np\n",
        "def loadGloveModel(gloveFile):\n",
        "    print (\"Loading Glove Model\")\n",
        "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
        "        content = f.readlines()\n",
        "    model = {}\n",
        "    for line in content:\n",
        "        splitLine = line.split()\n",
        "        word = splitLine[0]\n",
        "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
        "        model[word] = embedding\n",
        "    print (\"Done.\",len(model),\" words loaded!\")\n",
        "    return model\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def preprocess(raw_text):\n",
        "\n",
        "    # keep only words\n",
        "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
        "\n",
        "    # convert to lower case and split \n",
        "    words = letters_only_text.lower().split()\n",
        "\n",
        "    # remove stopwords\n",
        "    stopword_set = set(stopwords.words(\"german\"))\n",
        "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "def cosine_distance_between_two_words(word1, word2):\n",
        "    import scipy\n",
        "    return (1- scipy.spatial.distance.cosine(embed(word1), embed(word2)))\n",
        "\n",
        "def calculate_heat_matrix_for_two_sentences(s1,s2):\n",
        "    s1 = preprocess(s1)\n",
        "    s2 = preprocess(s2)\n",
        "    result_list = [[cosine_distance_between_two_words(word1, word2) for word2 in s2] for word1 in s1]\n",
        "    result_df = pd.DataFrame(result_list)\n",
        "    result_df.columns = s2\n",
        "    result_df.index = s1\n",
        "    return result_df\n",
        "\n",
        "def cosine_distance_wordembedding_method(s1, s2):\n",
        "    import scipy\n",
        "    vector_1 = np.mean([embed(word) for word in preprocess(s1)],axis=0)\n",
        "    vector_2 = np.mean([embed(word) for word in preprocess(s2)],axis=0)\n",
        "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
        "    return(round((1-cosine)*100,2))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt    \n",
        "\n",
        "def heat_map_matrix_between_two_sentences(s1,s2):\n",
        "    df = calculate_heat_matrix_for_two_sentences(s1,s2)\n",
        "    #fig, ax = plt.subplots(figsize=(5,5)) \n",
        "    #ax_blue = sns.heatmap(df, cmap=\"YlGnBu\")\n",
        "    #pyplot.show()\n",
        "    #ax_red = sns.heatmap(df)\n",
        "    #print()\n",
        "    return cosine_distance_wordembedding_method(s1, s2)\n",
        "#\n",
        "ss1 = 'ai artificial Intelligence maschinelles Lernen künstliche Intelligenz ki'\n",
        "ss2 = 'IBM und Uni Stuttgart kooperieren um die KI-Forschung in Deutschland voranzutreiben! #ibm4ins #KI #IBMWatson'\n",
        "#model = loadGloveModel(gloveFile)\n",
        "heat_map_matrix_between_two_sentences(ss1,ss2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kPxZcVtq5aOe",
        "outputId": "08f05885-7416-4b86-94f1-881a27705305",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73.76"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#gloveFile = \"glove.6B.50d.txt\"\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def preprocess_en(raw_text):\n",
        "\n",
        "    # keep only words\n",
        "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
        "\n",
        "    # convert to lower case and split \n",
        "    words = letters_only_text.lower().split()\n",
        "\n",
        "    # remove stopwords\n",
        "    stopword_set = set(stopwords.words(\"english\"))\n",
        "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "def cosine_distance_between_two_words_en(word1, word2):\n",
        "    import scipy\n",
        "    return (1- scipy.spatial.distance.cosine(embed_en(word1), embed_en(word2)))\n",
        "\n",
        "def calculate_heat_matrix_for_two_sentences_en(s1,s2):\n",
        "    s1 = preprocess_en(s1)\n",
        "    s2 = preprocess_en(s2)\n",
        "    result_list = [[cosine_distance_between_two_words_en(word1, word2) for word2 in s2] for word1 in s1]\n",
        "    result_df = pd.DataFrame(result_list)\n",
        "    result_df.columns = s2\n",
        "    result_df.index = s1\n",
        "    return result_df\n",
        "\n",
        "def cosine_distance_wordembedding_method_en(s1, s2):\n",
        "    import scipy\n",
        "    vector_1 = np.mean([embed_en(word) for word in preprocess_en(s1)],axis=0)\n",
        "    vector_2 = np.mean([embed_en(word) for word in preprocess_en(s2)],axis=0)\n",
        "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
        "    return(round((1-cosine)*100,2))\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def heat_map_matrix_between_two_sentences_en(s1,s2):\n",
        "    df = calculate_heat_matrix_for_two_sentences_en(s1,s2)\n",
        "    #fig, ax = plt.subplots(figsize=(5,5)) \n",
        "    #ax_blue = sns.heatmap(df, cmap=\"YlGnBu\")\n",
        "    # ax_red = sns.heatmap(df)\n",
        "    return cosine_distance_wordembedding_method_en(s1, s2)\n",
        "\n",
        "ss1 = 'ai artificial Intelligence machine learning'\n",
        "ss2 = 'How well do you understand the information &amp; knowledge within your organization?  Are you ready to leverage the benefits of #AI for you stakeholders? https://t.co/wpclrx0nDQ'\n",
        "\n",
        "#model = loadGloveModel(gloveFile)\n",
        "heat_map_matrix_between_two_sentences_en(ss1,ss2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Oe_lSHD65aOf",
        "outputId": "cc4f0302-95b9-420e-8564-d78a1d998419",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20929, 4)\n",
            "(1117, 4)\n",
            "   Unnamed: 0  ... lang\n",
            "0           0  ...   de\n",
            "1           1  ...   de\n",
            "2           2  ...   de\n",
            "3           3  ...   de\n",
            "7           7  ...   de\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "     Unnamed: 0  ... lang\n",
            "4             4  ...   en\n",
            "5             5  ...   en\n",
            "6             6  ...   en\n",
            "143         143  ...   en\n",
            "247         247  ...   en\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "#importing the ai dataset\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('ai_tweets_date.csv')\n",
        "df_de = df[df['lang']=='de']\n",
        "df_en = df[df['lang']=='en']\n",
        "\n",
        "print(df_de.shape)\n",
        "print(df_en.shape)\n",
        "print(df_de.head())\n",
        "print(df_en.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8jKikdOG5aOg",
        "outputId": "99728582-0537-4bc1-b442-3b1f9878ee64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ],
      "source": [
        "df_de.drop('Unnamed: 0', inplace = True, axis = 1)\n",
        "df_en.drop('Unnamed: 0', inplace = True, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bEcbadIU5aOg"
      },
      "outputs": [],
      "source": [
        "de_tweets = df_de['all_text'].to_numpy()\n",
        "en_tweets = df_en['all_text'].to_numpy()\n",
        "de_date = df_de['created_at_string'].to_numpy()\n",
        "en_date = df_en['created_at_string'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECNFOEzU5aOh",
        "outputId": "f32cd9cb-e854-4386-e267-0753c6b99b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        }
      ],
      "source": [
        "#calc. the relevance score for each de tweet and storing it in de_dist\n",
        "\n",
        "de_list = []\n",
        "ss1 = 'ai artificial Intelligence maschinelles Lernen künstliche Intelligenz ki'\n",
        "for i in range(len(de_tweets)):\n",
        "    ss2 = de_tweets[i]\n",
        "    x = heat_map_matrix_between_two_sentences(ss1,ss2)\n",
        "    de_list.append(x)\n",
        "    \n",
        "de_relevance = np.array(de_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Yjvaah35aOi"
      },
      "outputs": [],
      "source": [
        "#calc. the relevance score for each eng tweet and storing it in en_dist\n",
        "\n",
        "en_list = []\n",
        "ss1 = 'ai artificial Intelligence machine learning'\n",
        "for i in range(len(en_tweets)):\n",
        "    ss2 = en_tweets[i]\n",
        "    x = heat_map_matrix_between_two_sentences_en(ss1,ss2)\n",
        "    en_list.append(x)\n",
        "    \n",
        "en_relevance = np.array(en_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcoC0YPY5aOi"
      },
      "outputs": [],
      "source": [
        "de_result = pd.DataFrame(list(zip(de_date,de_tweets,de_relevance)), columns = ['date','all_text','relevance'])\n",
        "en_result = pd.DataFrame(list(zip(en_date,en_tweets,en_relevance)), columns = ['date','all_text','relevance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRKRiFRO5aOi"
      },
      "outputs": [],
      "source": [
        "frames = [de_result, en_result]\n",
        "\n",
        "df_comb = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unq-u7BL5aOj"
      },
      "outputs": [],
      "source": [
        "df_comb = df_comb.sort_values(by=\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHrRf3Fp5aOj"
      },
      "outputs": [],
      "source": [
        "df_comb.drop('all_text', inplace = True, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgpMxrvU5aOj"
      },
      "outputs": [],
      "source": [
        "df_comb = df_comb.groupby('date').mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzwvEt4N5aOj"
      },
      "outputs": [],
      "source": [
        "df_comb.to_csv('ai_relevance.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Refernce : [link text](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n",
        "\n"
      ],
      "metadata": {
        "id": "jyW13r4KVZpR"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Embedding.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}